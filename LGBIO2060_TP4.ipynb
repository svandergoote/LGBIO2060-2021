{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LGBIO2060_TP4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHSFUXVQbYAvTaY9OwhfUR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svandergoote/LGBIO2060-2021/blob/TP4/LGBIO2060_TP4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q9FHZbAXUjK"
      },
      "source": [
        "# LGBIO2060 Exercice session 4\n",
        "\n",
        "#Hidden markov model\n",
        "\n",
        "__Authors:__ Simon Vandergooten, Cl√©mence Vandamme\n",
        "\n",
        "__Content inspired from__: Neuromatch Academy github.com/NeuromatchAcademy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A4jkLCkXr0B"
      },
      "source": [
        "##Introduction and context\n",
        "In this tutorial we will make the previous models a little more complex. Until now, we have used binary or continuous variables whose state was fixed in time. It is easy to see that this greatly limits our ability to represent real systems. One way to get closer to reality would be to allow our hidden states to change their values. \n",
        "\n",
        "Let's take the example of the fish. Previously they were either on the right or on the left. But now, we will add to the model the fact that fishes can switch side. \n",
        "\n",
        "It is exactly what **Hidden markov models** permits. The Markov property specifies that you can fully encapsulate the important properties of a system based on its current state at the current time, any previous history does not matter. It is memoryless. Back to the fishes, it means that the school of fish is only at one position at a time and the probability of them being on the left or on the right at time *t* depends only on the state at time *t-1* and the probabilities of transition from one state to another. It can be represented with a matrix called the **transition matrix**.\n",
        "\n",
        "<img alt='Solution hint' align='left' width=650 height=300 src=https://raw.githubusercontent.com/svandergoote/LGBIO2060-2021/master/Solutions/HMM.png>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neP8J8DKm2k6"
      },
      "source": [
        "We can use linear algebra to express the probabilities of the current state.\n",
        "\n",
        "$$P_i = [P(state_i = right), P(state_i = left) ] $$\n",
        "\n",
        "To compute the vector of probabilities of the state at the time i+1, we can use linear algebra and multiply our vector of the probabilities of the current state with the transition matrix.\n",
        "\n",
        "$$P_{i+1} = P_{i} T$$\n",
        "where $T$ is our transition matrix.\n",
        "\n",
        "This is the same formula for every step, which allows us to get the probabilities for a time more than 1 step in advance easily. If we started at $i=0$ and wanted to look at the probabilities at step $i=2$, we could do:\n",
        "\n",
        "\\begin{align*}\n",
        "P_{1} &= P_{0}T\\\\\n",
        "P_{2} &= P_{1}T = P_{0}TT = P_{0}T^2\\\\\n",
        "\\end{align*}\n",
        "\n",
        "So, every time we take a further step we can just multiply with the transition matrix again. So, the probability vector of states at j timepoints after the current state at timepoint i is equal to the probability vector at timepoint i times the transition matrix raised to the jth power.\n",
        "$$P_{i + j} = P_{i}T^j $$\n",
        "\n",
        "By the end of this tutorial, you should be able to:\n",
        "- Describe how the hidden states in a Hidden Markov model evolve over time, both in words, mathematically, and in code\n",
        "- Estimate hidden states from data using forward inference in a Hidden Markov model\n",
        "- Describe how measurement noise and state transition probabilities affect uncertainty in predictions in the future and the ability to estimate hidden states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9haFetXebje"
      },
      "source": [
        "# Section 1: Binary HMM with Gaussian measurements\n",
        "We will represent the hidden state *s* of the fish with the values +1 and -1 for the right and left position respectively. \n",
        "\n",
        "The probability of switching to state $s_t=j$ from the previous state $s_{t-1}=i$ is the conditional probability distribution $p(s_t = j| s_{t-1} = i)$.\n",
        "\n",
        "In our case, we can summarize those transition probabilities into the **Transition matrix T**.\n",
        "\n",
        "\\begin{align*}\n",
        "T = \\begin{bmatrix}p(s_t = +1 | s_{t-1} = +1) & p(s_t = -1 | s_{t-1} = +1)\\\\p(s_t = +1 | s_{t-1} = -1)& p(s_t = -1 | s_{t-1} = -1)\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "###Measurements\n",
        "In a Hidden Markov model, we cannot directly observe the latent states $s_t$. Instead we get noisy measurements $m_t \\sim p(m|s_t)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWqpbUheiMJ5"
      },
      "source": [
        "##Coding exercice 1.1 : Hidden states\n",
        "You will first implement the function `generate_state`. This function will create the vector of states *S* based on the model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC2tO6qgXQXr"
      },
      "source": [
        "def generate_state(switch_proba, start_proba, n_states):\n",
        "  '''\n",
        "  Create an HMM binary state variable.\n",
        "  Args:\n",
        "    switch_proba (array): the probabilities to switch from states. [rightToLeft, LeftToRight]\n",
        "    start_proba (array): the initial probabilities of being on each side. [p_right, p_left]\n",
        "    n_states (int): the number of time steps\n",
        "\n",
        "  Returns:\n",
        "    S (array): the vector of state for each time step.\n",
        "  '''\n",
        "  ##########################\n",
        "  ##### Your code here #####\n",
        "  ##########################\n",
        "  #Initialize S\n",
        "  S = ...\n",
        "\n",
        "  #Step 1: Initial state (Hint: np.random.choice)\n",
        "  S[0] = ...\n",
        "\n",
        "  #Step 2: Transition matrix\n",
        "  T = ...\n",
        "  \n",
        "  #Step 3: Iterate on each time step to find the new state s[t] based on S[t-1]\n",
        "\n",
        "\n",
        "\n",
        "  return S\n",
        "\n",
        "#Set random seed\n",
        "np.random.seed(54)\n",
        "\n",
        "#Set parameters of HMM\n",
        "switch_proba = np.array([0.4, 0.7])\n",
        "start_proba = np.array([1, 0]) #The initial state is right (+1)\n",
        "n_states = 50\n",
        "\n",
        "#Generate the hidden states vector\n",
        "S = generate_state(switch_proba, start_proba, n_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlMpx1F1rxRJ"
      },
      "source": [
        "##Coding exercice 1.2 : Noisy measurements\n",
        "Now that you have created the hidden states, you will create the noisy Gaussian measurements vector *M* from it.\n",
        "\n",
        "Recall that in reality we don't have access to the hidden states but only to noisy measurements that give us information about those hidden states we want to infer.\n",
        "\n",
        "You will implement the function `sample` that generates samples $m_t$ based on the hidden states $s_t$. \n",
        "- if hidden state $s_t = +1 $ : $m_t \\sim N(+1,\\sigma)$\n",
        "- if hidden state $s_t = -1 $ : $m_t \\sim N(-1,\\sigma)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-P-EAX-rvUz"
      },
      "source": [
        "def sample(means, var, S):\n",
        "  '''\n",
        "  Create a Gaussian measurement from HMM states\n",
        "\n",
        "    Args: \n",
        "      means (array): Mean mesurement for each state [right, left].\n",
        "      var (float): Variance of measurement models. Same for each state.\n",
        "      S (array): The series of hidden states.\n",
        "    \n",
        "    Returns:\n",
        "      M (array): The series of measurements.\n",
        "  '''\n",
        "  ##########################\n",
        "  ##### Your code here #####\n",
        "  ##########################\n",
        "\n",
        "  #Calculate measurements conditioned on the latent states (Hint: np.random.normal)\n",
        "  \n",
        "\n",
        "  return M"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}