{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LGBIO2060_TP4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNolO2REmxmpmVo4I7q1d4a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svandergoote/LGBIO2060-2021/blob/TP4/LGBIO2060_TP4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q9FHZbAXUjK"
      },
      "source": [
        "# LGBIO2060 Exercice session 4\n",
        "\n",
        "#Hidden markov model\n",
        "\n",
        "__Authors:__ Simon Vandergooten, Cl√©mence Vandamme\n",
        "\n",
        "__Content inspired from__: Neuromatch Academy github.com/NeuromatchAcademy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A4jkLCkXr0B"
      },
      "source": [
        "##Introduction and context\n",
        "In this tutorial we will make the previous models a little more complex. Until now, we have used binary or continuous variables whose state was fixed in time. It is easy to see that this greatly limits our ability to represent real systems. One way to get closer to reality would be to allow our hidden states to change their values. \n",
        "\n",
        "Let's take the example of the fish. Previously they were either on the right or on the left. But now, we will add to the model the fact that fishes can switch side. \n",
        "\n",
        "It is exactly what **Hidden markov models** permits. The Markov property specifies that you can fully encapsulate the important properties of a system based on its current state at the current time, any previous history does not matter. It is memoryless. Back to the fishes, it means that the school of fish is only at one position at a time and the probability of them being on the left or on the right at time *t* depends only on the state at time *t-1* and the probabilities of transition from one state to another. It can be represented with a matrix called the **transition matrix**.\n",
        "\n",
        "<img alt='Solution hint' align='left' width=650 height=300 src=https://raw.githubusercontent.com/svandergoote/LGBIO2060-2021/master/Solutions/HMM.png>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neP8J8DKm2k6"
      },
      "source": [
        "We can use linear algebra to express the probabilities of the current state.\n",
        "\n",
        "$$P_i = [P(state_i = right), P(state_i = left) ] $$\n",
        "\n",
        "To compute the vector of probabilities of the state at the time i+1, we can use linear algebra and multiply our vector of the probabilities of the current state with the transition matrix.\n",
        "\n",
        "$$P_{i+1} = P_{i} T$$\n",
        "where $T$ is our transition matrix.\n",
        "\n",
        "This is the same formula for every step, which allows us to get the probabilities for a time more than 1 step in advance easily. If we started at $i=0$ and wanted to look at the probabilities at step $i=2$, we could do:\n",
        "\n",
        "\\begin{align*}\n",
        "P_{1} &= P_{0}T\\\\\n",
        "P_{2} &= P_{1}T = P_{0}TT = P_{0}T^2\\\\\n",
        "\\end{align*}\n",
        "\n",
        "So, every time we take a further step we can just multiply with the transition matrix again. So, the probability vector of states at j timepoints after the current state at timepoint i is equal to the probability vector at timepoint i times the transition matrix raised to the jth power.\n",
        "$$P_{i + j} = P_{i}T^j $$\n",
        "\n",
        "By the end of this tutorial, you should be able to:\n",
        "- Describe how the hidden states in a Hidden Markov model evolve over time, both in words, mathematically, and in code\n",
        "- Estimate hidden states from data using forward inference in a Hidden Markov model\n",
        "- Describe how measurement noise and state transition probabilities affect uncertainty in predictions in the future and the ability to estimate hidden states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9haFetXebje"
      },
      "source": [
        "# Section 1: Binary HMM with Gaussian measurements\n",
        "We will represent the hidden state *s* of the fish with the values +1 and -1 for the right and left position respectively. \n",
        "\n",
        "The probability of switching to state $s_t=j$ from the previous state $s_{t-1}=i$ is the conditional probability distribution $p(s_t = j| s_{t-1} = i)$.\n",
        "\n",
        "In our case, we can summarize those transition probabilities into the **Transition matrix T**.\n",
        "\n",
        "\\begin{align*}\n",
        "T = \\begin{bmatrix}p(s_t = +1 | s_{t-1} = +1) & p(s_t = -1 | s_{t-1} = +1)\\\\p(s_t = +1 | s_{t-1} = -1)& p(s_t = -1 | s_{t-1} = -1)\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "###Measurements\n",
        "In a Hidden Markov model, we cannot directly observe the latent states $s_t$. Instead we get noisy measurements $m_t \\sim p(m|s_t)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "89xniMe31KWO"
      },
      "source": [
        "#@title Imports\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from scipy import stats\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from collections import namedtuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "xlKicbtl1RPO"
      },
      "source": [
        "#@title Figure Settings\n",
        "# import ipywidgets as widgets       # interactive display\n",
        "from IPython.html import widgets\n",
        "from ipywidgets import interactive, interact, HBox, Layout,VBox\n",
        "from IPython.display import HTML\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "fFxuEpkr1WJw"
      },
      "source": [
        "# @title Plotting Functions\n",
        "\n",
        "def plot_hmm1(model, states, measurements, flag_m=True):\n",
        "  \"\"\"Plots HMM states and measurements for 1d states and measurements.\n",
        "\n",
        "  Args:\n",
        "    model (hmmlearn model):               hmmlearn model used to get state means.\n",
        "    states (numpy array of floats):       Samples of the states.\n",
        "    measurements (numpy array of floats): Samples of the states.\n",
        "  \"\"\"\n",
        "  T = states.shape[0]\n",
        "  nsteps = states.size\n",
        "  aspect_ratio = 2\n",
        "  fig, ax1 = plt.subplots(figsize=(8,4))\n",
        "  states_forplot = list(map(lambda s: model.means[s], states))\n",
        "  ax1.step(np.arange(nstep), states_forplot, \"-\", where=\"mid\", alpha=1.0, c=\"green\")\n",
        "  ax1.set_xlabel(\"Time\")\n",
        "  ax1.set_ylabel(\"Latent State\", c=\"green\")\n",
        "  ax1.set_yticks([-1, 1])\n",
        "  ax1.set_yticklabels([\"-1\", \"+1\"])\n",
        "  ax1.set_xticks(np.arange(0,T,10))\n",
        "  ymin = min(measurements)\n",
        "  ymax = max(measurements)\n",
        "\n",
        "  ax2 = ax1.twinx()\n",
        "  ax2.set_ylabel(\"Measurements\", c=\"crimson\")\n",
        "\n",
        "  # show measurement gaussian\n",
        "  if flag_m:\n",
        "    ax2.plot([T,T],ax2.get_ylim(), color=\"maroon\", alpha=0.6)\n",
        "    for i in range(model.n_components):\n",
        "      mu = model.means[i]\n",
        "      scale = np.sqrt(model.vars[i])\n",
        "      rv = stats.norm(mu, scale)\n",
        "      num_points = 50\n",
        "      domain = np.linspace(mu-3*scale, mu+3*scale, num_points)\n",
        "\n",
        "      left = np.repeat(float(T), num_points)\n",
        "      # left = np.repeat(0.0, num_points)\n",
        "      offset = rv.pdf(domain)\n",
        "      offset *= T / 15\n",
        "      lbl = \"measurement\" if i == 0 else \"\"\n",
        "      # ax2.fill_betweenx(domain, left, left-offset, alpha=0.3, lw=2, color=\"maroon\", label=lbl)\n",
        "      ax2.fill_betweenx(domain, left+offset, left, alpha=0.3, lw=2, color=\"maroon\", label=lbl)\n",
        "      ax2.scatter(np.arange(nstep), measurements, c=\"crimson\", s=4)\n",
        "      ax2.legend(loc=\"upper left\")\n",
        "    ax1.set_ylim(ax2.get_ylim())\n",
        "  plt.show(fig)\n",
        "\n",
        "\n",
        "def plot_marginal_seq(predictive_probs, switch_prob):\n",
        "  \"\"\"Plots the sequence of marginal predictive distributions.\n",
        "\n",
        "    Args:\n",
        "      predictive_probs (list of numpy vectors): sequence of predictive probability vectors\n",
        "      switch_prob (float):                      Probability of switching states.\n",
        "  \"\"\"\n",
        "  T = len(predictive_probs)\n",
        "  prob_neg = [p_vec[0] for p_vec in predictive_probs]\n",
        "  prob_pos = [p_vec[1] for p_vec in predictive_probs]\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(np.arange(T), prob_neg, color=\"blue\")\n",
        "  ax.plot(np.arange(T), prob_pos, color=\"orange\")\n",
        "  ax.legend([\n",
        "    \"prob in state -1\", \"prob in state 1\"\n",
        "  ])\n",
        "  ax.text(T/2, 0.05, \"switching probability={}\".format(switch_prob), fontsize=12,\n",
        "          bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.6))\n",
        "  ax.set_xlabel(\"Time\")\n",
        "  ax.set_ylabel(\"Probability\")\n",
        "  ax.set_title(\"Forgetting curve in a changing world\")\n",
        "  #ax.set_aspect(aspect_ratio)\n",
        "  plt.show(fig)\n",
        "\n",
        "def plot_evidence_vs_noevidence(posterior_matrix, predictive_probs):\n",
        "  \"\"\"Plots the average posterior probabilities with evidence v.s. no evidence\n",
        "\n",
        "  Args:\n",
        "    posterior_matrix: (2d numpy array of floats): The posterior probabilities in state 1 from evidence (samples, time)\n",
        "    predictive_probs (numpy array of floats):  Predictive probabilities in state 1 without evidence\n",
        "  \"\"\"\n",
        "  nsample, T = posterior_matrix.shape\n",
        "  posterior_mean = posterior_matrix.mean(axis=0)\n",
        "  fig, ax = plt.subplots(1)\n",
        "  # ax.plot([0.0, T],[0.5, 0.5], color=\"red\", linestyle=\"dashed\")\n",
        "  ax.plot([0.0, T],[0., 0.], color=\"red\", linestyle=\"dashed\")\n",
        "  ax.plot(np.arange(T), predictive_probs, c=\"orange\", linewidth=2, label=\"No evidence\")\n",
        "  ax.scatter(np.tile(np.arange(T), (nsample, 1)), posterior_matrix, s=0.8, c=\"green\", alpha=0.3, label=\"With evidence(Sample)\")\n",
        "  ax.plot(np.arange(T), posterior_mean, c='green', linewidth=2, label=\"With evidence(Average)\")\n",
        "  ax.legend()\n",
        "  ax.set_yticks([0.0, 0.25, 0.5, 0.75, 1.0])\n",
        "  ax.set_xlabel(\"Time\")\n",
        "  ax.set_ylabel(\"Probability in State +1\")\n",
        "  ax.set_title(\"Gain confidence with evidence\")\n",
        "  plt.show(fig)\n",
        "\n",
        "\n",
        "def plot_forward_inference(model, states, measurements, states_inferred,\n",
        "                           predictive_probs, likelihoods, posterior_probs,\n",
        "                           t=None,\n",
        "                           flag_m=True, flag_d=True, flag_pre=True, flag_like=True, flag_post=True,\n",
        "                           ):\n",
        "  \"\"\"Plot ground truth state sequence with noisy measurements, and ground truth states v.s. inferred ones\n",
        "\n",
        "      Args:\n",
        "          model (instance of hmmlearn.GaussianHMM): an instance of HMM\n",
        "          states (numpy vector): vector of 0 or 1(int or Bool), the sequences of true latent states\n",
        "          measurements (numpy vector of numpy vector): the un-flattened Gaussian measurements at each time point, element has size (1,)\n",
        "          states_inferred (numpy vector): vector of 0 or 1(int or Bool), the sequences of inferred latent states\n",
        "  \"\"\"\n",
        "  T = states.shape[0]\n",
        "  if t is None:\n",
        "    t = T-1\n",
        "  nsteps = states.size\n",
        "  fig, ax1 = plt.subplots(figsize=(11,6))\n",
        "  # inferred states\n",
        "  #ax1.step(np.arange(nstep)[:t+1], states_forplot[:t+1], \"-\", where=\"mid\", alpha=1.0, c=\"orange\", label=\"inferred\")\n",
        "  # true states\n",
        "  states_forplot = list(map(lambda s: model.means[s], states))\n",
        "  ax1.step(np.arange(nstep)[:t+1], states_forplot[:t+1], \"-\", where=\"mid\", alpha=1.0, c=\"green\", label=\"true\")\n",
        "  ax1.step(np.arange(nstep)[t+1:], states_forplot[t+1:], \"-\", where=\"mid\", alpha=0.3, c=\"green\", label=\"\")\n",
        "  # Posterior curve\n",
        "  delta = model.means[1] - model.means[0]\n",
        "  states_interpolation = model.means[0] + delta * posterior_probs[:,1]\n",
        "  if flag_post:\n",
        "    ax1.step(np.arange(nstep)[:t+1], states_interpolation[:t+1], \"-\", where=\"mid\", c=\"grey\", label=\"posterior\")\n",
        "\n",
        "  ax1.set_xlabel(\"Time\")\n",
        "  ax1.set_ylabel(\"Latent State\", c=\"green\")\n",
        "  ax1.set_yticks([-1, 1])\n",
        "  ax1.set_yticklabels([\"-1\", \"+1\"])\n",
        "  ax1.legend(bbox_to_anchor=(0,1.02,0.2,0.1), borderaxespad=0, ncol=2)\n",
        "\n",
        "\n",
        "\n",
        "  ax2 = ax1.twinx()\n",
        "  ax2.set_ylim(\n",
        "      min(-1.2, np.min(measurements)),\n",
        "      max(1.2, np.max(measurements))\n",
        "      )\n",
        "  if flag_d:\n",
        "    ax2.scatter(np.arange(nstep)[:t+1], measurements[:t+1], c=\"crimson\", s=4, label=\"measurement\")\n",
        "    ax2.set_ylabel(\"Measurements\", c=\"crimson\")\n",
        "\n",
        "  # show measurement distributions\n",
        "  if flag_m:\n",
        "    for i in range(model.n_components):\n",
        "      mu = model.means[i]\n",
        "      scale = np.sqrt(model.vars[i])\n",
        "      rv = stats.norm(mu, scale)\n",
        "      num_points = 50\n",
        "      domain = np.linspace(mu-3*scale, mu+3*scale, num_points)\n",
        "\n",
        "      left = np.repeat(float(T), num_points)\n",
        "      offset = rv.pdf(domain)\n",
        "      offset *= T /15\n",
        "      # lbl = \"measurement\" if i == 0 else \"\"\n",
        "      lbl = \"\"\n",
        "      # ax2.fill_betweenx(domain, left, left-offset, alpha=0.3, lw=2, color=\"maroon\", label=lbl)\n",
        "      ax2.fill_betweenx(domain, left+offset, left, alpha=0.3, lw=2, color=\"maroon\", label=lbl)\n",
        "  ymin, ymax = ax2.get_ylim()\n",
        "  width = 0.1 * (ymax-ymin) / 2.0\n",
        "  centers = [-1.0, 1.0]\n",
        "  bar_scale = 15\n",
        "\n",
        "  # Predictions\n",
        "  data = predictive_probs\n",
        "  if flag_pre:\n",
        "    for i in range(model.n_components):\n",
        "      domain = np.array([centers[i]-1.5*width, centers[i]-0.5*width])\n",
        "      left = np.array([t,t])\n",
        "      offset = np.array([data[t,i]]*2)\n",
        "      offset *= bar_scale\n",
        "      lbl = \"todays prior\" if i == 0 else \"\"\n",
        "      ax2.fill_betweenx(domain, left+offset, left, alpha=0.3, lw=2, color=\"dodgerblue\", label=lbl)\n",
        "\n",
        "  # Likelihoods\n",
        "  # data = np.stack([likelihoods, 1.0-likelihoods],axis=-1)\n",
        "  data = likelihoods\n",
        "  data /= np.sum(data,axis=-1, keepdims=True)\n",
        "  if flag_like:\n",
        "    for i in range(model.n_components):\n",
        "      domain = np.array([centers[i]+0.5*width, centers[i]+1.5*width])\n",
        "      left = np.array([t,t])\n",
        "      offset = np.array([data[t,i]]*2)\n",
        "      offset *= bar_scale\n",
        "      lbl = \"likelihood\" if i == 0 else \"\"\n",
        "      ax2.fill_betweenx(domain, left+offset, left, alpha=0.3, lw=2, color=\"crimson\", label=lbl)\n",
        "  # Posteriors\n",
        "  data = posterior_probs\n",
        "  if flag_post:\n",
        "    for i in range(model.n_components):\n",
        "      domain = np.array([centers[i]-0.5*width, centers[i]+0.5*width])\n",
        "      left = np.array([t,t])\n",
        "      offset = np.array([data[t,i]]*2)\n",
        "      offset *= bar_scale\n",
        "      lbl = \"posterior\" if i == 0 else \"\"\n",
        "      ax2.fill_betweenx(domain, left+offset, left, alpha=0.3, lw=2, color=\"grey\", label=lbl)\n",
        "  if t<T-1:\n",
        "    ax2.plot([t,t],ax2.get_ylim(), color='black',alpha=0.6)\n",
        "  if flag_pre or flag_like or flag_post:\n",
        "    ax2.plot([t,t],ax2.get_ylim(), color='black',alpha=0.6)\n",
        "\n",
        "    ax2.legend(bbox_to_anchor=(0.4,1.02,0.6, 0.1), borderaxespad=0, ncol=4)\n",
        "  ax1.set_ylim(ax2.get_ylim())\n",
        "  return fig\n",
        "  # plt.show(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWqpbUheiMJ5"
      },
      "source": [
        "##Coding exercice 1.1 : Hidden states\n",
        "You will first implement the function `generate_state`. This function will create the vector of states *S* based on the model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC2tO6qgXQXr"
      },
      "source": [
        "def generate_state(switch_proba, start_proba, n_states):\n",
        "  '''\n",
        "  Create an HMM binary state variable.\n",
        "  Args:\n",
        "    switch_proba (array): the probabilities to switch from states. [rightToLeft, LeftToRight]\n",
        "    start_proba (array): the initial probabilities of being on each side. [p_right, p_left]\n",
        "    n_states (int): the number of time steps\n",
        "\n",
        "  Returns:\n",
        "    S (array): the vector of state for each time step.\n",
        "  '''\n",
        "  ##########################\n",
        "  ##### Your code here #####\n",
        "  ##########################\n",
        "  #Initialize S\n",
        "  S = ...\n",
        "\n",
        "  #Step 1: Initial state (Hint: np.random.choice)\n",
        "  S[0] = ...\n",
        "\n",
        "  #Step 2: Transition matrix\n",
        "  T = ...\n",
        "  \n",
        "  #Step 3: Iterate on each time step to find the new state s[t] based on S[t-1]\n",
        "\n",
        "\n",
        "\n",
        "  return S\n",
        "\n",
        "#Set random seed\n",
        "np.random.seed(54)\n",
        "\n",
        "#Set parameters of HMM\n",
        "switch_proba = np.array([0.4, 0.7])\n",
        "start_proba = np.array([1, 0]) #The initial state is right (+1)\n",
        "n_states = 50\n",
        "\n",
        "#Generate the hidden states vector\n",
        "S = generate_state(switch_proba, start_proba, n_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlMpx1F1rxRJ"
      },
      "source": [
        "##Coding exercice 1.2 : Noisy measurements\n",
        "Now that you have created the hidden states, you will create the noisy Gaussian measurements vector *M* from it.\n",
        "\n",
        "Recall that in reality we don't have access to the hidden states but only to noisy measurements that give us information about those hidden states we want to infer.\n",
        "\n",
        "You will implement the function `sample` that generates samples $m_t$ based on the hidden states $s_t$. \n",
        "- if hidden state $s_t = +1 $ : $m_t \\sim N(+1,\\sigma)$\n",
        "- if hidden state $s_t = -1 $ : $m_t \\sim N(-1,\\sigma)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-P-EAX-rvUz"
      },
      "source": [
        "def sample(means, var, S):\n",
        "  '''\n",
        "  Create a Gaussian measurement from HMM states\n",
        "\n",
        "    Args: \n",
        "      means (array): Mean mesurement for each state [right, left].\n",
        "      var (float): Variance of measurement models. Same for each state.\n",
        "      S (array): The series of hidden states.\n",
        "    \n",
        "    Returns:\n",
        "      M (array): The series of measurements.\n",
        "  '''\n",
        "  ##########################\n",
        "  ##### Your code here #####\n",
        "  ##########################\n",
        "\n",
        "  #Calculate measurements conditioned on the latent states (Hint: np.random.normal)\n",
        "  \n",
        "\n",
        "  return M"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}